{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LightGCN_with_clustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "# os.symlink('/content/mnt/My Drive/Colab_Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)  # or append(nb_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ShioIx2lB_f",
        "outputId": "f2eb319a-c202-4a32-f6e7-e2f32da05159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/mnt; to attempt to forcibly remount, call drive.mount(\"/content/mnt\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change these filepaths to change what data the model uses\n",
        "TRAIN_FILEPATH = '/gcn_formatted_train_dense_clustered.txt'\n",
        "TEST_FILEPATH = '/gcn_formatted_test_dense_clustered.txt'"
      ],
      "metadata": {
        "id": "eFthPUU1LzL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## parse.py"
      ],
      "metadata": {
        "id": "YvD48o8yMrvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Mar 1, 2020\n",
        "Pytorch Implementation of LightGCN in\n",
        "Xiangnan He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n",
        "\n",
        "@author: Jianbai Ye (gusye@mail.ustc.edu.cn)\n",
        "'''\n",
        "import argparse\n",
        "\n",
        "# hyperparameters set as default\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Go lightGCN\")\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument('--bpr_batch', type=int,default=1024,\n",
        "                        help=\"the batch size for bpr loss training procedure\")\n",
        "    parser.add_argument('--recdim', type=int,default=64,\n",
        "                        help=\"the embedding size of lightGCN\")\n",
        "    parser.add_argument('--layer', type=int,default=10,\n",
        "                        help=\"the layer num of lightGCN\")\n",
        "    parser.add_argument('--lr', type=float,default=0.001,\n",
        "                        help=\"the learning rate\")\n",
        "    parser.add_argument('--decay', type=float,default=1e-4,\n",
        "                        help=\"the weight decay for l2 normalizaton\")\n",
        "    parser.add_argument('--dropout', type=int,default=0,\n",
        "                        help=\"using the dropout or not\")\n",
        "    parser.add_argument('--keepprob', type=float,default=0.6,\n",
        "                        help=\"the batch size for bpr loss training procedure\")\n",
        "    parser.add_argument('--a_fold', type=int,default=100,\n",
        "                        help=\"the fold num used to split large adj matrix, like gowalla\")\n",
        "    parser.add_argument('--testbatch', type=int,default=3,\n",
        "                        help=\"the batch size of users for testing\")\n",
        "    parser.add_argument('--dataset', type=str,default='gowalla',\n",
        "                        help=\"available datasets: [lastfm, gowalla, yelp2018, amazon-book]\")\n",
        "    parser.add_argument('--path', type=str,default=\"./checkpoints\",\n",
        "                        help=\"path to save weights\")\n",
        "    parser.add_argument('--topks', nargs='?',default=\"[20]\",\n",
        "                        help=\"@k test list\")\n",
        "    parser.add_argument('--tensorboard', type=int,default=1,\n",
        "                        help=\"enable tensorboard\")\n",
        "    parser.add_argument('--comment', type=str,default=\"lgn\")\n",
        "    parser.add_argument('--load', type=int,default=0)\n",
        "    parser.add_argument('--epochs', type=int,default=200)\n",
        "    parser.add_argument('--multicore', type=int, default=0, help='whether we use multiprocessing or not in test')\n",
        "    parser.add_argument('--pretrain', type=int, default=0, help='whether we use pretrained weight or not')\n",
        "    parser.add_argument('--seed', type=int, default=2020, help='random seed')\n",
        "    parser.add_argument('--model', type=str, default='lgn', help='rec-model, support [mf, lgn]')\n",
        "    return parser.parse_args()"
      ],
      "metadata": {
        "id": "lJ5Zu2Z8MzMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## args for gowalla \n",
        "cd code && python main.py --decay=1e-4 --lr=0.001 --layer=3 --seed=2020 --dataset=\"gowalla\" --topks=\"[20]\" --recdim=64"
      ],
      "metadata": {
        "id": "mmHEpM1FwtrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()"
      ],
      "metadata": {
        "id": "o2LReXFOx7dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## args for yelp2018\n",
        "cd code && python main.py --decay=1e-4 --lr=0.001 --layer=3 --seed=2020 --dataset=\"yelp2018\" --topks=\"[20]\" --recdim=64"
      ],
      "metadata": {
        "id": "h_GQx3UjxjYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()\n",
        "args.dataset = \"yelp2018\""
      ],
      "metadata": {
        "id": "_thxuKwWwtW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## args for meme dataset"
      ],
      "metadata": {
        "id": "2NXu3lmSgIIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()\n",
        "args.dataset = \"meme\""
      ],
      "metadata": {
        "id": "2v4SNxyBgHR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## world.py"
      ],
      "metadata": {
        "id": "fJflRiGhG8zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Mar 1, 2020\n",
        "Pytorch Implementation of LightGCN in\n",
        "Xiangnan He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n",
        "\n",
        "@author: Jianbai Ye (gusye@mail.ustc.edu.cn)\n",
        "'''\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "import torch\n",
        "from enum import Enum\n",
        "# from parse import parse_args\n",
        "import multiprocessing\n",
        "\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "# ROOT_PATH = \"/Users/gus/Desktop/light-gcn\"\n",
        "ROOT_PATH = \"/content/mnt/Shareddrives/247 Project/\"\n",
        "CODE_PATH = join(ROOT_PATH, 'code')\n",
        "DATA_PATH = join(ROOT_PATH, 'data')\n",
        "BOARD_PATH = join(CODE_PATH, 'runs')\n",
        "FILE_PATH = join(CODE_PATH, 'checkpoints')\n",
        "import sys\n",
        "sys.path.append(join(CODE_PATH, 'sources'))\n",
        "\n",
        "\n",
        "if not os.path.exists(FILE_PATH):\n",
        "    os.makedirs(FILE_PATH, exist_ok=True)\n",
        "\n",
        "\n",
        "config = {}\n",
        "all_dataset = ['lastfm', 'gowalla', 'yelp2018', 'amazon-book', 'meme']\n",
        "all_models  = ['mf', 'lgn']\n",
        "# config['batch_size'] = 4096\n",
        "config['bpr_batch_size'] = args.bpr_batch\n",
        "config['latent_dim_rec'] = args.recdim\n",
        "config['lightGCN_n_layers']= args.layer\n",
        "config['dropout'] = args.dropout\n",
        "config['keep_prob']  = args.keepprob\n",
        "config['A_n_fold'] = args.a_fold\n",
        "config['test_u_batch_size'] = args.testbatch\n",
        "config['multicore'] = args.multicore\n",
        "config['lr'] = args.lr\n",
        "config['decay'] = args.decay\n",
        "config['pretrain'] = args.pretrain\n",
        "config['A_split'] = False\n",
        "config['bigdata'] = False\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "GPU = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if GPU else \"cpu\")\n",
        "CORES = multiprocessing.cpu_count() // 2\n",
        "seed = args.seed\n",
        "\n",
        "dataset = args.dataset\n",
        "model_name = args.model\n",
        "if dataset not in all_dataset:\n",
        "    raise NotImplementedError(f\"Haven't supported {dataset} yet!, try {all_dataset}\")\n",
        "if model_name not in all_models:\n",
        "    raise NotImplementedError(f\"Haven't supported {model_name} yet!, try {all_models}\")\n",
        "\n",
        "\n",
        "TRAIN_epochs = args.epochs\n",
        "LOAD = args.load\n",
        "PATH = args.path\n",
        "topks = eval(args.topks)\n",
        "tensorboard = args.tensorboard\n",
        "comment = args.comment\n",
        "# let pandas shut up\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "\n",
        "def cprint(words : str):\n",
        "    print(f\"\\033[0;30;43m{words}\\033[0m\")\n",
        "\n",
        "logo = r\"\"\"\n",
        "██╗      ██████╗ ███╗   ██╗\n",
        "██║     ██╔════╝ ████╗  ██║\n",
        "██║     ██║  ███╗██╔██╗ ██║\n",
        "██║     ██║   ██║██║╚██╗██║\n",
        "███████╗╚██████╔╝██║ ╚████║\n",
        "╚══════╝ ╚═════╝ ╚═╝  ╚═══╝\n",
        "\"\"\"\n",
        "# font: ANSI Shadow\n",
        "# refer to http://patorjk.com/software/taag/#p=display&f=ANSI%20Shadow&t=Sampling\n",
        "# print(logo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH_PUGK7OQIf",
        "outputId": "d5752c1f-a148-461c-cb53-6d429c4006aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataloader.py"
      ],
      "metadata": {
        "id": "cOLHMdcjHML8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on Mar 1, 2020\n",
        "Pytorch Implementation of LightGCN in\n",
        "Xiangnan He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n",
        "\n",
        "@author: Shuxian Bi (stanbi@mail.ustc.edu.cn),Jianbai Ye (gusye@mail.ustc.edu.cn)\n",
        "Design Dataset here\n",
        "Every dataset's index has to start at 0\n",
        "\"\"\"\n",
        "import os\n",
        "from os.path import join\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy.sparse as sp\n",
        "# import world\n",
        "# from world import cprint\n",
        "# from time import time\n",
        "import time\n",
        "\n",
        "class BasicDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        print(\"init dataset\")\n",
        "    \n",
        "    @property\n",
        "    def n_users(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    @property\n",
        "    def m_items(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    @property\n",
        "    def trainDataSize(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    @property\n",
        "    def testDict(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    @property\n",
        "    def allPos(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def getUserItemFeedback(self, users, items):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def getUserPosItems(self, users):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def getUserNegItems(self, users):\n",
        "        \"\"\"\n",
        "        not necessary for large dataset\n",
        "        it's stupid to return all neg items in super large dataset\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def getSparseGraph(self):\n",
        "        \"\"\"\n",
        "        build a graph in torch.sparse.IntTensor.\n",
        "        Details in NGCF's matrix form\n",
        "        A = \n",
        "            |I,   R|\n",
        "            |R^T, I|\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class LastFM(BasicDataset):\n",
        "    \"\"\"\n",
        "    Dataset type for pytorch \\n\n",
        "    Incldue graph information\n",
        "    LastFM dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, path=\"../data/lastfm\"):\n",
        "        # train or test\n",
        "        cprint(\"loading [last fm]\")\n",
        "        self.mode_dict = {'train':0, \"test\":1}\n",
        "        self.mode    = self.mode_dict['train']\n",
        "        # self.n_users = 1892\n",
        "        # self.m_items = 4489\n",
        "        trainData = pd.read_table(join(path, 'data1.txt'), header=None)\n",
        "        # print(trainData.head())\n",
        "        testData  = pd.read_table(join(path, 'test1.txt'), header=None)\n",
        "        # print(testData.head())\n",
        "        trustNet  = pd.read_table(join(path, 'trustnetwork.txt'), header=None).to_numpy()\n",
        "        # print(trustNet[:5])\n",
        "        trustNet -= 1\n",
        "        trainData-= 1\n",
        "        testData -= 1\n",
        "        self.trustNet  = trustNet\n",
        "        self.trainData = trainData\n",
        "        self.testData  = testData\n",
        "        self.trainUser = np.array(trainData[:][0])\n",
        "        self.trainUniqueUsers = np.unique(self.trainUser)\n",
        "        self.trainItem = np.array(trainData[:][1])\n",
        "        # self.trainDataSize = len(self.trainUser)\n",
        "        self.testUser  = np.array(testData[:][0])\n",
        "        self.testUniqueUsers = np.unique(self.testUser)\n",
        "        self.testItem  = np.array(testData[:][1])\n",
        "        self.Graph = None\n",
        "        print(f\"LastFm Sparsity : {(len(self.trainUser) + len(self.testUser))/self.n_users/self.m_items}\")\n",
        "        \n",
        "        # (users,users)\n",
        "        self.socialNet    = csr_matrix((np.ones(len(trustNet)), (trustNet[:,0], trustNet[:,1]) ), shape=(self.n_users,self.n_users))\n",
        "        # (users,items), bipartite graph\n",
        "        self.UserItemNet  = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem) ), shape=(self.n_users,self.m_items)) \n",
        "        \n",
        "        # pre-calculate\n",
        "        self._allPos = self.getUserPosItems(list(range(self.n_users)))\n",
        "        self.allNeg = []\n",
        "        allItems    = set(range(self.m_items))\n",
        "        for i in range(self.n_users):\n",
        "            pos = set(self._allPos[i])\n",
        "            neg = allItems - pos\n",
        "            self.allNeg.append(np.array(list(neg)))\n",
        "        self.__testDict = self.__build_test()\n",
        "\n",
        "    @property\n",
        "    def n_users(self):\n",
        "        return 1892\n",
        "    \n",
        "    @property\n",
        "    def m_items(self):\n",
        "        return 4489\n",
        "    \n",
        "    @property\n",
        "    def trainDataSize(self):\n",
        "        return len(self.trainUser)\n",
        "    \n",
        "    @property\n",
        "    def testDict(self):\n",
        "        return self.__testDict\n",
        "\n",
        "    @property\n",
        "    def allPos(self):\n",
        "        return self._allPos\n",
        "\n",
        "    def getSparseGraph(self):\n",
        "        if self.Graph is None:\n",
        "            user_dim = torch.LongTensor(self.trainUser)\n",
        "            item_dim = torch.LongTensor(self.trainItem)\n",
        "            \n",
        "            first_sub = torch.stack([user_dim, item_dim + self.n_users])\n",
        "            second_sub = torch.stack([item_dim+self.n_users, user_dim])\n",
        "            index = torch.cat([first_sub, second_sub], dim=1)\n",
        "            data = torch.ones(index.size(-1)).int()\n",
        "            self.Graph = torch.sparse.IntTensor(index, data, torch.Size([self.n_users+self.m_items, self.n_users+self.m_items]))\n",
        "            dense = self.Graph.to_dense()\n",
        "            D = torch.sum(dense, dim=1).float()\n",
        "            D[D==0.] = 1.\n",
        "            D_sqrt = torch.sqrt(D).unsqueeze(dim=0)\n",
        "            dense = dense/D_sqrt\n",
        "            dense = dense/D_sqrt.t()\n",
        "            index = dense.nonzero()\n",
        "            data  = dense[dense >= 1e-9]\n",
        "            assert len(index) == len(data)\n",
        "            self.Graph = torch.sparse.FloatTensor(index.t(), data, torch.Size([self.n_users+self.m_items, self.n_users+self.m_items]))\n",
        "            self.Graph = self.Graph.coalesce().to(device)\n",
        "        return self.Graph\n",
        "\n",
        "    def __build_test(self):\n",
        "        \"\"\"\n",
        "        return:\n",
        "            dict: {user: [items]}\n",
        "        \"\"\"\n",
        "        test_data = {}\n",
        "        for i, item in enumerate(self.testItem):\n",
        "            user = self.testUser[i]\n",
        "            if test_data.get(user):\n",
        "                test_data[user].append(item)\n",
        "            else:\n",
        "                test_data[user] = [item]\n",
        "        return test_data\n",
        "    \n",
        "    def getUserItemFeedback(self, users, items):\n",
        "        \"\"\"\n",
        "        users:\n",
        "            shape [-1]\n",
        "        items:\n",
        "            shape [-1]\n",
        "        return:\n",
        "            feedback [-1]\n",
        "        \"\"\"\n",
        "        # print(self.UserItemNet[users, items])\n",
        "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1, ))\n",
        "    \n",
        "    def getUserPosItems(self, users):\n",
        "        posItems = []\n",
        "        for user in users:\n",
        "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
        "        return posItems\n",
        "    \n",
        "    def getUserNegItems(self, users):\n",
        "        negItems = []\n",
        "        for user in users:\n",
        "            negItems.append(self.allNeg[user])\n",
        "        return negItems\n",
        "            \n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        user = self.trainUniqueUsers[index]\n",
        "        # return user_id and the positive items of the user\n",
        "        return user\n",
        "    \n",
        "    def switch2test(self):\n",
        "        \"\"\"\n",
        "        change dataset mode to offer test data to dataloader\n",
        "        \"\"\"\n",
        "        self.mode = self.mode_dict['test']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.trainUniqueUsers)\n",
        "\n",
        "class Loader(BasicDataset):\n",
        "    \"\"\"\n",
        "    Dataset type for pytorch \\n\n",
        "    Incldue graph information\n",
        "    gowalla dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,config = config,path=\"../data/gowalla\"):\n",
        "        # train or test\n",
        "        cprint(f'loading [{path}]')\n",
        "        self.split = config['A_split']\n",
        "        self.folds = config['A_n_fold']\n",
        "        self.mode_dict = {'train': 0, \"test\": 1}\n",
        "        self.mode = self.mode_dict['train']\n",
        "        self.n_user = 0\n",
        "        self.m_item = 0\n",
        "        # train_file = path + '/train.txt'\n",
        "        # print(path)\n",
        "        train_file = path + TRAIN_FILEPATH\n",
        "        # test_file = path + '/test.txt'\n",
        "        test_file = path + TEST_FILEPATH\n",
        "        self.path = path\n",
        "        trainUniqueUsers, trainItem, trainUser = [], [], []\n",
        "        testUniqueUsers, testItem, testUser = [], [], []\n",
        "        self.traindataSize = 0\n",
        "        self.testDataSize = 0\n",
        "\n",
        "        with open(train_file) as f:\n",
        "            print(\"inside training file: \", train_file)\n",
        "            for l in f.readlines():\n",
        "                if len(l) > 0:\n",
        "                    l = l.strip('\\n').split(' ')\n",
        "                    items = [int(i) for i in l[1:]]\n",
        "                    uid = int(l[0])\n",
        "                    trainUniqueUsers.append(uid)\n",
        "                    trainUser.extend([uid] * len(items))\n",
        "                    trainItem.extend(items)\n",
        "                    self.m_item = max(self.m_item, max(items))\n",
        "                    self.n_user = max(self.n_user, uid)\n",
        "                    self.traindataSize += len(items)\n",
        "        self.trainUniqueUsers = np.array(trainUniqueUsers)\n",
        "        self.trainUser = np.array(trainUser)\n",
        "        self.trainItem = np.array(trainItem)\n",
        "        with open(test_file) as f:\n",
        "            for l in f.readlines():\n",
        "                if len(l) > 0:\n",
        "                    l = l.strip('\\n').split(' ')\n",
        "                    items = [int(i) for i in l[1:]]\n",
        "                    uid = int(l[0])\n",
        "                    testUniqueUsers.append(uid)\n",
        "                    testUser.extend([uid] * len(items))\n",
        "                    testItem.extend(items)\n",
        "                    self.m_item = max(self.m_item, max(items))\n",
        "                    self.n_user = max(self.n_user, uid)\n",
        "                    self.testDataSize += len(items)\n",
        "        self.m_item += 1\n",
        "        self.n_user += 1\n",
        "        self.testUniqueUsers = np.array(testUniqueUsers)\n",
        "        self.testUser = np.array(testUser)\n",
        "        self.testItem = np.array(testItem)\n",
        "        \n",
        "        self.Graph = None\n",
        "        print(f\"{self.trainDataSize} interactions for training\")\n",
        "        print(f\"{self.testDataSize} interactions for testing\")\n",
        "        print(f\"{dataset} Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n",
        "\n",
        "        # (users,items), bipartite graph\n",
        "        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n",
        "                                      shape=(self.n_user, self.m_item))\n",
        "        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n",
        "        self.users_D[self.users_D == 0.] = 1\n",
        "        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n",
        "        self.items_D[self.items_D == 0.] = 1.\n",
        "        # pre-calculate\n",
        "        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n",
        "        self.__testDict = self.__build_test()\n",
        "        print(f\"{dataset} is ready to go\")\n",
        "\n",
        "    @property\n",
        "    def n_users(self):\n",
        "        return self.n_user\n",
        "    \n",
        "    @property\n",
        "    def m_items(self):\n",
        "        return self.m_item\n",
        "    \n",
        "    @property\n",
        "    def trainDataSize(self):\n",
        "        return self.traindataSize\n",
        "    \n",
        "    @property\n",
        "    def testDict(self):\n",
        "        return self.__testDict\n",
        "\n",
        "    @property\n",
        "    def allPos(self):\n",
        "        return self._allPos\n",
        "\n",
        "    def _split_A_hat(self,A):\n",
        "        A_fold = []\n",
        "        fold_len = (self.n_users + self.m_items) // self.folds\n",
        "        for i_fold in range(self.folds):\n",
        "            start = i_fold*fold_len\n",
        "            if i_fold == self.folds - 1:\n",
        "                end = self.n_users + self.m_items\n",
        "            else:\n",
        "                end = (i_fold + 1) * fold_len\n",
        "            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(device))\n",
        "        return A_fold\n",
        "\n",
        "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
        "        coo = X.tocoo().astype(np.float32)\n",
        "        row = torch.Tensor(coo.row).long()\n",
        "        col = torch.Tensor(coo.col).long()\n",
        "        index = torch.stack([row, col])\n",
        "        data = torch.FloatTensor(coo.data)\n",
        "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
        "        \n",
        "    def getSparseGraph(self):\n",
        "        print(\"loading adjacency matrix\")\n",
        "        if self.Graph is None:\n",
        "            # try:\n",
        "            #     pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n",
        "            #     print(\"successfully loaded...\")\n",
        "            #     norm_adj = pre_adj_mat\n",
        "            # except :\n",
        "            print(\"generating adjacency matrix\")\n",
        "            s = time.time()\n",
        "            adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
        "            adj_mat = adj_mat.tolil()\n",
        "            R = self.UserItemNet.tolil()\n",
        "            adj_mat[:self.n_users, self.n_users:] = R\n",
        "            adj_mat[self.n_users:, :self.n_users] = R.T\n",
        "            adj_mat = adj_mat.todok()\n",
        "            # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
        "            \n",
        "            rowsum = np.array(adj_mat.sum(axis=1))\n",
        "            d_inv = np.power(rowsum, -0.5).flatten()\n",
        "            d_inv[np.isinf(d_inv)] = 0.\n",
        "            d_mat = sp.diags(d_inv)\n",
        "            \n",
        "            norm_adj = d_mat.dot(adj_mat)\n",
        "            norm_adj = norm_adj.dot(d_mat)\n",
        "            norm_adj = norm_adj.tocsr()\n",
        "            end = time.time()\n",
        "            print(f\"costing {end-s}s, saved norm_mat...\")\n",
        "            sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n",
        "\n",
        "            if self.split == True:\n",
        "                self.Graph = self._split_A_hat(norm_adj)\n",
        "                print(\"done split matrix\")\n",
        "            else:\n",
        "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
        "                self.Graph = self.Graph.coalesce().to(device)\n",
        "                print(\"don't split the matrix\")\n",
        "        return self.Graph\n",
        "\n",
        "    def __build_test(self):\n",
        "        \"\"\"\n",
        "        return:\n",
        "            dict: {user: [items]}\n",
        "        \"\"\"\n",
        "        test_data = {}\n",
        "        for i, item in enumerate(self.testItem):\n",
        "            user = self.testUser[i]\n",
        "            if test_data.get(user):\n",
        "                test_data[user].append(item)\n",
        "            else:\n",
        "                test_data[user] = [item]\n",
        "        return test_data\n",
        "\n",
        "    def getUserItemFeedback(self, users, items):\n",
        "        \"\"\"\n",
        "        users:\n",
        "            shape [-1]\n",
        "        items:\n",
        "            shape [-1]\n",
        "        return:\n",
        "            feedback [-1]\n",
        "        \"\"\"\n",
        "        # print(self.UserItemNet[users, items])\n",
        "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n",
        "\n",
        "    def getUserPosItems(self, users):\n",
        "        posItems = []\n",
        "        for user in users:\n",
        "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
        "        return posItems\n",
        "\n",
        "    # def getUserNegItems(self, users):\n",
        "    #     negItems = []\n",
        "    #     for user in users:\n",
        "    #         negItems.append(self.allNeg[user])\n",
        "    #     return negItems"
      ],
      "metadata": {
        "id": "zJYGCbRIWEQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meme dataset implementation"
      ],
      "metadata": {
        "id": "ciWhFoJSdbJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "acjMVcd9IQfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model.py"
      ],
      "metadata": {
        "id": "BdXnQiWyZeHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on Mar 1, 2020\n",
        "Pytorch Implementation of LightGCN in\n",
        "Xiangnan He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n",
        "\n",
        "@author: Jianbai Ye (gusye@mail.ustc.edu.cn)\n",
        "\n",
        "Define models here\n",
        "\"\"\"\n",
        "# import world\n",
        "import torch\n",
        "# from dataloader import BasicDataset\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BasicModel(nn.Module):    \n",
        "    def __init__(self):\n",
        "        super(BasicModel, self).__init__()\n",
        "    \n",
        "    def getUsersRating(self, users):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "class PairWiseModel(BasicModel):\n",
        "    def __init__(self):\n",
        "        super(PairWiseModel, self).__init__()\n",
        "    def bpr_loss(self, users, pos, neg):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            users: users list \n",
        "            pos: positive items for corresponding users\n",
        "            neg: negative items for corresponding users\n",
        "        Return:\n",
        "            (log-loss, l2-loss)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "class PureMF(BasicModel):\n",
        "    def __init__(self, \n",
        "                 config:dict, \n",
        "                 dataset:BasicDataset):\n",
        "        super(PureMF, self).__init__()\n",
        "        self.num_users  = dataset.n_users\n",
        "        self.num_items  = dataset.m_items\n",
        "        self.latent_dim = config['latent_dim_rec']\n",
        "        self.f = nn.Sigmoid()\n",
        "        self.__init_weight()\n",
        "        \n",
        "    def __init_weight(self):\n",
        "        self.embedding_user = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "        print(\"using Normal distribution N(0,1) initialization for PureMF\")\n",
        "        \n",
        "    def getUsersRating(self, users):\n",
        "        users = users.long()\n",
        "        users_emb = self.embedding_user(users)\n",
        "        items_emb = self.embedding_item.weight\n",
        "        scores = torch.matmul(users_emb, items_emb.t())\n",
        "        return self.f(scores)\n",
        "    \n",
        "    def bpr_loss(self, users, pos, neg):\n",
        "        users_emb = self.embedding_user(users.long())\n",
        "        pos_emb   = self.embedding_item(pos.long())\n",
        "        neg_emb   = self.embedding_item(neg.long())\n",
        "        pos_scores= torch.sum(users_emb*pos_emb, dim=1)\n",
        "        neg_scores= torch.sum(users_emb*neg_emb, dim=1)\n",
        "        loss = torch.mean(nn.functional.softplus(neg_scores - pos_scores))\n",
        "        reg_loss = (1/2)*(users_emb.norm(2).pow(2) + \n",
        "                          pos_emb.norm(2).pow(2) + \n",
        "                          neg_emb.norm(2).pow(2))/float(len(users))\n",
        "        return loss, reg_loss\n",
        "        \n",
        "    def forward(self, users, items):\n",
        "        users = users.long()\n",
        "        items = items.long()\n",
        "        users_emb = self.embedding_user(users)\n",
        "        items_emb = self.embedding_item(items)\n",
        "        scores = torch.sum(users_emb*items_emb, dim=1)\n",
        "        return self.f(scores)\n",
        "\n",
        "class LightGCN(BasicModel):\n",
        "    def __init__(self, \n",
        "                 config:dict, \n",
        "                 dataset:BasicDataset):\n",
        "        super(LightGCN, self).__init__()\n",
        "        self.config = config\n",
        "        self.dataset : BasicDataset = dataset\n",
        "        self.__init_weight()\n",
        "\n",
        "    def __init_weight(self):\n",
        "        self.num_users  = self.dataset.n_users\n",
        "        self.num_items  = self.dataset.m_items\n",
        "        self.latent_dim = self.config['latent_dim_rec']\n",
        "        self.n_layers = self.config['lightGCN_n_layers']\n",
        "        self.keep_prob = self.config['keep_prob']\n",
        "        self.A_split = self.config['A_split']\n",
        "        self.embedding_user = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "        if self.config['pretrain'] == 0:\n",
        "#             nn.init.xavier_uniform_(self.embedding_user.weight, gain=1)\n",
        "#             nn.init.xavier_uniform_(self.embedding_item.weight, gain=1)\n",
        "#             print('use xavier initilizer')\n",
        "# random normal init seems to be a better choice when lightGCN actually don't use any non-linear activation function\n",
        "            nn.init.normal_(self.embedding_user.weight, std=0.1)\n",
        "            nn.init.normal_(self.embedding_item.weight, std=0.1)\n",
        "            cprint('use NORMAL distribution initilizer')\n",
        "        else:\n",
        "            self.embedding_user.weight.data.copy_(torch.from_numpy(self.config['user_emb']))\n",
        "            self.embedding_item.weight.data.copy_(torch.from_numpy(self.config['item_emb']))\n",
        "            print('use pretarined data')\n",
        "        self.f = nn.Sigmoid()\n",
        "        self.Graph = self.dataset.getSparseGraph()\n",
        "        print(f\"lgn is already to go(dropout:{self.config['dropout']})\")\n",
        "\n",
        "        # print(\"save_txt\")\n",
        "    def __dropout_x(self, x, keep_prob):\n",
        "        size = x.size()\n",
        "        index = x.indices().t()\n",
        "        values = x.values()\n",
        "        random_index = torch.rand(len(values)) + keep_prob\n",
        "        random_index = random_index.int().bool()\n",
        "        index = index[random_index]\n",
        "        values = values[random_index]/keep_prob\n",
        "        g = torch.sparse.FloatTensor(index.t(), values, size)\n",
        "        return g\n",
        "    \n",
        "    def __dropout(self, keep_prob):\n",
        "        if self.A_split:\n",
        "            graph = []\n",
        "            for g in self.Graph:\n",
        "                graph.append(self.__dropout_x(g, keep_prob))\n",
        "        else:\n",
        "            graph = self.__dropout_x(self.Graph, keep_prob)\n",
        "        return graph\n",
        "    \n",
        "    def computer(self):\n",
        "        \"\"\"\n",
        "        propagate methods for lightGCN\n",
        "        \"\"\"       \n",
        "        users_emb = self.embedding_user.weight\n",
        "        items_emb = self.embedding_item.weight\n",
        "        all_emb = torch.cat([users_emb, items_emb])\n",
        "        #   torch.split(all_emb , [self.num_users, self.num_items])\n",
        "        embs = [all_emb]\n",
        "        if self.config['dropout']:\n",
        "            if self.training:\n",
        "                print(\"droping\")\n",
        "                g_droped = self.__dropout(self.keep_prob)\n",
        "            else:\n",
        "                g_droped = self.Graph        \n",
        "        else:\n",
        "            g_droped = self.Graph    \n",
        "        \n",
        "        for layer in range(self.n_layers):\n",
        "            if self.A_split:\n",
        "                temp_emb = []\n",
        "                for f in range(len(g_droped)):\n",
        "                    temp_emb.append(torch.sparse.mm(g_droped[f], all_emb))\n",
        "                side_emb = torch.cat(temp_emb, dim=0)\n",
        "                all_emb = side_emb\n",
        "            else:\n",
        "                all_emb = torch.sparse.mm(g_droped, all_emb)\n",
        "            embs.append(all_emb)\n",
        "        embs = torch.stack(embs, dim=1)\n",
        "        #print(embs.size())\n",
        "        light_out = torch.mean(embs, dim=1)\n",
        "        users, items = torch.split(light_out, [self.num_users, self.num_items])\n",
        "        return users, items\n",
        "    \n",
        "    def getUsersRating(self, users):\n",
        "        all_users, all_items = self.computer()\n",
        "        users_emb = all_users[users.long()]\n",
        "        items_emb = all_items\n",
        "        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n",
        "        return rating\n",
        "    \n",
        "    def getEmbedding(self, users, pos_items, neg_items):\n",
        "        all_users, all_items = self.computer()\n",
        "        users_emb = all_users[users]\n",
        "        pos_emb = all_items[pos_items]\n",
        "        neg_emb = all_items[neg_items]\n",
        "        users_emb_ego = self.embedding_user(users)\n",
        "        pos_emb_ego = self.embedding_item(pos_items)\n",
        "        neg_emb_ego = self.embedding_item(neg_items)\n",
        "        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego\n",
        "    \n",
        "    def bpr_loss(self, users, pos, neg):\n",
        "        (users_emb, pos_emb, neg_emb, \n",
        "        userEmb0,  posEmb0, negEmb0) = self.getEmbedding(users.long(), pos.long(), neg.long())\n",
        "        reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
        "                         posEmb0.norm(2).pow(2)  +\n",
        "                         negEmb0.norm(2).pow(2))/float(len(users))\n",
        "        pos_scores = torch.mul(users_emb, pos_emb)\n",
        "        pos_scores = torch.sum(pos_scores, dim=1)\n",
        "        neg_scores = torch.mul(users_emb, neg_emb)\n",
        "        neg_scores = torch.sum(neg_scores, dim=1)\n",
        "        \n",
        "        loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
        "        \n",
        "        return loss, reg_loss\n",
        "       \n",
        "    def forward(self, users, items):\n",
        "        # compute embedding\n",
        "        all_users, all_items = self.computer()\n",
        "        # print('forward')\n",
        "        #all_users, all_items = self.computer()\n",
        "        users_emb = all_users[users]\n",
        "        items_emb = all_items[items]\n",
        "        inner_pro = torch.mul(users_emb, items_emb)\n",
        "        gamma     = torch.sum(inner_pro, dim=1)\n",
        "        return gamma"
      ],
      "metadata": {
        "id": "nSFLqCXTZelv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## util.py"
      ],
      "metadata": {
        "id": "63XSB01sG3_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Mar 1, 2020\n",
        "Pytorch Implementation of LightGCN in\n",
        "Xiangnan He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n",
        "\n",
        "@author: Jianbai Ye (gusye@mail.ustc.edu.cn)\n",
        "'''\n",
        "# import world\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "from torch import log\n",
        "# from dataloader import BasicDataset\n",
        "# from time import time\n",
        "import time\n",
        "# from model import LightGCN\n",
        "# from model import PairWiseModel\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import random\n",
        "import os\n",
        "# try:\n",
        "#     from cppimport import imp_from_filepath\n",
        "#     from os.path import join, dirname\n",
        "#     path = join(dirname(__file__), \"sources/sampling.cpp\")\n",
        "#     sampling = imp_from_filepath(path)\n",
        "#     sampling.seed(world.seed)\n",
        "#     sample_ext = True\n",
        "# except:\n",
        "#     world.cprint(\"Cpp extension not loaded\")\n",
        "#     sample_ext = False\n",
        "cprint(\"Cpp extension not loaded\")\n",
        "sample_ext = False\n",
        "\n",
        "class BPRLoss:\n",
        "    def __init__(self,\n",
        "                 recmodel : PairWiseModel,\n",
        "                 config : dict):\n",
        "        self.model = recmodel\n",
        "        self.weight_decay = config['decay']\n",
        "        self.lr = config['lr']\n",
        "        self.opt = optim.Adam(recmodel.parameters(), lr=self.lr)\n",
        "\n",
        "    def stageOne(self, users, pos, neg):\n",
        "        loss, reg_loss = self.model.bpr_loss(users, pos, neg)\n",
        "        reg_loss = reg_loss*self.weight_decay\n",
        "        loss = loss + reg_loss\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        return loss.cpu().item()\n",
        "\n",
        "\n",
        "def UniformSample_original(dataset, neg_ratio = 1):\n",
        "    dataset : BasicDataset\n",
        "    allPos = dataset.allPos\n",
        "    start = time.time()\n",
        "    # if sample_ext:\n",
        "    #     S = sampling.sample_negative(dataset.n_users, dataset.m_items,\n",
        "    #                                  dataset.trainDataSize, allPos, neg_ratio)\n",
        "    # else:\n",
        "    #     S = UniformSample_original_python(dataset)\n",
        "    S = UniformSample_original_python(dataset)\n",
        "    return S\n",
        "\n",
        "def UniformSample_original_python(dataset):\n",
        "    \"\"\"\n",
        "    the original impliment of BPR Sampling in LightGCN\n",
        "    :return:\n",
        "        np.array\n",
        "    \"\"\"\n",
        "    total_start =  time.time()\n",
        "    dataset : BasicDataset\n",
        "    user_num = dataset.trainDataSize\n",
        "    users = np.random.randint(0, dataset.n_users, user_num)\n",
        "    allPos = dataset.allPos\n",
        "    S = []\n",
        "    sample_time1 = 0.\n",
        "    sample_time2 = 0.\n",
        "    for i, user in enumerate(users):\n",
        "        start = time.time()\n",
        "        posForUser = allPos[user]\n",
        "        if len(posForUser) == 0:\n",
        "            continue\n",
        "        sample_time2 += time.time() - start\n",
        "        posindex = np.random.randint(0, len(posForUser))\n",
        "        positem = posForUser[posindex]\n",
        "        while True:\n",
        "            negitem = np.random.randint(0, dataset.m_items)\n",
        "            if negitem in posForUser:\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        S.append([user, positem, negitem])\n",
        "        end = time.time()\n",
        "        sample_time1 += end - start\n",
        "    total = time.time() - total_start\n",
        "    return np.array(S)\n",
        "\n",
        "# ===================end samplers==========================\n",
        "# =====================utils====================================\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def getFileName():\n",
        "    if model_name == 'mf':\n",
        "        file = f\"mf-{dataset}-{config['latent_dim_rec']}.pth.tar\"\n",
        "    elif model_name == 'lgn':\n",
        "        file = f\"lgn-{dataset}-{config['lightGCN_n_layers']}-{config['latent_dim_rec']}.pth.tar\"\n",
        "    return os.path.join(FILE_PATH,file)\n",
        "\n",
        "def minibatch(*tensors, **kwargs):\n",
        "\n",
        "    batch_size = kwargs.get('batch_size', config['bpr_batch_size'])\n",
        "\n",
        "    if len(tensors) == 1:\n",
        "        tensor = tensors[0]\n",
        "        for i in range(0, len(tensor), batch_size):\n",
        "            yield tensor[i:i + batch_size]\n",
        "    else:\n",
        "        for i in range(0, len(tensors[0]), batch_size):\n",
        "            yield tuple(x[i:i + batch_size] for x in tensors)\n",
        "\n",
        "\n",
        "def shuffle(*arrays, **kwargs):\n",
        "\n",
        "    require_indices = kwargs.get('indices', False)\n",
        "\n",
        "    if len(set(len(x) for x in arrays)) != 1:\n",
        "        raise ValueError('All inputs to shuffle must have '\n",
        "                         'the same length.')\n",
        "\n",
        "    shuffle_indices = np.arange(len(arrays[0]))\n",
        "    np.random.shuffle(shuffle_indices)\n",
        "\n",
        "    if len(arrays) == 1:\n",
        "        result = arrays[0][shuffle_indices]\n",
        "    else:\n",
        "        result = tuple(x[shuffle_indices] for x in arrays)\n",
        "\n",
        "    if require_indices:\n",
        "        return result, shuffle_indices\n",
        "    else:\n",
        "        return result\n",
        "\n",
        "\n",
        "class timer:\n",
        "    \"\"\"\n",
        "    Time context manager for code block\n",
        "        with timer():\n",
        "            do something\n",
        "        timer.get()\n",
        "    \"\"\"\n",
        "    # from time import time\n",
        "    import time\n",
        "    TAPE = [-1]  # global time record\n",
        "    NAMED_TAPE = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def get():\n",
        "        if len(timer.TAPE) > 1:\n",
        "            return timer.TAPE.pop()\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    @staticmethod\n",
        "    def dict(select_keys=None):\n",
        "        hint = \"|\"\n",
        "        if select_keys is None:\n",
        "            for key, value in timer.NAMED_TAPE.items():\n",
        "                hint = hint + f\"{key}:{value:.2f}|\"\n",
        "        else:\n",
        "            for key in select_keys:\n",
        "                value = timer.NAMED_TAPE[key]\n",
        "                hint = hint + f\"{key}:{value:.2f}|\"\n",
        "        return hint\n",
        "\n",
        "    @staticmethod\n",
        "    def zero(select_keys=None):\n",
        "        if select_keys is None:\n",
        "            for key, value in timer.NAMED_TAPE.items():\n",
        "                timer.NAMED_TAPE[key] = 0\n",
        "        else:\n",
        "            for key in select_keys:\n",
        "                timer.NAMED_TAPE[key] = 0\n",
        "\n",
        "    def __init__(self, tape=None, **kwargs):\n",
        "        if kwargs.get('name'):\n",
        "            timer.NAMED_TAPE[kwargs['name']] = timer.NAMED_TAPE[\n",
        "                kwargs['name']] if timer.NAMED_TAPE.get(kwargs['name']) else 0.\n",
        "            self.named = kwargs['name']\n",
        "            if kwargs.get(\"group\"):\n",
        "                #TODO: add group function\n",
        "                pass\n",
        "        else:\n",
        "            self.named = False\n",
        "            self.tape = tape or timer.TAPE\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.named:\n",
        "            timer.NAMED_TAPE[self.named] += time.time() - self.start\n",
        "        else:\n",
        "            self.tape.append(time.time() - self.start)\n",
        "\n",
        "\n",
        "# ====================Metrics==============================\n",
        "# =========================================================\n",
        "def RecallPrecision_ATk(test_data, r, k):\n",
        "    \"\"\"\n",
        "    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n",
        "    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n",
        "    k : top-k\n",
        "    \"\"\"\n",
        "    right_pred = r[:, :k].sum(1)\n",
        "    precis_n = k\n",
        "    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n",
        "    recall = np.sum(right_pred/recall_n)\n",
        "    precis = np.sum(right_pred)/precis_n\n",
        "    return {'recall': recall, 'precision': precis}\n",
        "\n",
        "\n",
        "def MRRatK_r(r, k):\n",
        "    \"\"\"\n",
        "    Mean Reciprocal Rank\n",
        "    \"\"\"\n",
        "    pred_data = r[:, :k]\n",
        "    scores = np.log2(1./np.arange(1, k+1))\n",
        "    pred_data = pred_data/scores\n",
        "    pred_data = pred_data.sum(1)\n",
        "    return np.sum(pred_data)\n",
        "\n",
        "def NDCGatK_r(test_data,r,k):\n",
        "    \"\"\"\n",
        "    Normalized Discounted Cumulative Gain\n",
        "    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n",
        "    \"\"\"\n",
        "    assert len(r) == len(test_data)\n",
        "    pred_data = r[:, :k]\n",
        "\n",
        "    test_matrix = np.zeros((len(pred_data), k))\n",
        "    for i, items in enumerate(test_data):\n",
        "        length = k if k <= len(items) else len(items)\n",
        "        test_matrix[i, :length] = 1\n",
        "    max_r = test_matrix\n",
        "    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n",
        "    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n",
        "    dcg = np.sum(dcg, axis=1)\n",
        "    idcg[idcg == 0.] = 1.\n",
        "    ndcg = dcg/idcg\n",
        "    ndcg[np.isnan(ndcg)] = 0.\n",
        "    return np.sum(ndcg)\n",
        "\n",
        "def AUC(all_item_scores, dataset, test_data):\n",
        "    \"\"\"\n",
        "        design for a single user\n",
        "    \"\"\"\n",
        "    dataset : BasicDataset\n",
        "    r_all = np.zeros((dataset.m_items, ))\n",
        "    r_all[test_data] = 1\n",
        "    r = r_all[all_item_scores >= 0]\n",
        "    test_item_scores = all_item_scores[all_item_scores >= 0]\n",
        "    return roc_auc_score(r, test_item_scores)\n",
        "\n",
        "def getLabel(test_data, pred_data):\n",
        "    r = []\n",
        "    for i in range(len(test_data)):\n",
        "        groundTrue = test_data[i]\n",
        "        predictTopK = pred_data[i]\n",
        "        pred = list(map(lambda x: x in groundTrue, predictTopK))\n",
        "        pred = np.array(pred).astype(\"float\")\n",
        "        r.append(pred)\n",
        "    return np.array(r).astype('float')\n",
        "\n",
        "# ====================end Metrics=============================\n",
        "# ========================================================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0Cf7dAXV942",
        "outputId": "9c8db139-6873-4b37-d054-f1718dacd258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;43mCpp extension not loaded\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## register.py"
      ],
      "metadata": {
        "id": "-r0NO5n4ajFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import world\n",
        "# import dataloader\n",
        "# import model\n",
        "# import utils\n",
        "from pprint import pprint\n",
        "\n",
        "if dataset in ['gowalla', 'yelp2018', 'amazon-book', 'meme']:\n",
        "    print('gowalla or yelp or amazon or meme dataset selected')\n",
        "    # dataset = Loader(path=DATA_PATH+'/'+dataset)\n",
        "    dataset = Loader(path=DATA_PATH)\n",
        "elif dataset == 'lastfm':\n",
        "    print('last fm dataset selected')\n",
        "    dataset = LastFM()\n",
        "# elif dataset == 'meme':\n",
        "#     print(\"meme dataset selected\")\n",
        "#     dataset = MemeLoader()\n",
        "\n",
        "print('===========config================')\n",
        "pprint(config)\n",
        "print(\"cores for test:\", CORES)\n",
        "print(\"comment:\", comment)\n",
        "print(\"tensorboard:\", tensorboard)\n",
        "print(\"LOAD:\", LOAD)\n",
        "print(\"Weight path:\", PATH)\n",
        "print(\"Test Topks:\", topks)\n",
        "print(\"using bpr loss\")\n",
        "print('===========end===================')\n",
        "\n",
        "MODELS = {\n",
        "    'mf': PureMF,\n",
        "    'lgn': LightGCN\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kh0Ut1AajNg",
        "outputId": "fc828f96-eacd-4f5a-fdfc-7786e68e5b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gowalla or yelp or amazon or meme dataset selected\n",
            "\u001b[0;30;43mloading [/content/mnt/Shareddrives/247 Project/data]\u001b[0m\n",
            "inside training file:  /content/mnt/Shareddrives/247 Project/data/gcn_formatted_train_dense_clustered.txt\n",
            "12136 interactions for training\n",
            "825 interactions for testing\n",
            "meme Sparsity : 0.03574462217319361\n",
            "meme is ready to go\n",
            "===========config================\n",
            "{'A_n_fold': 100,\n",
            " 'A_split': False,\n",
            " 'bigdata': False,\n",
            " 'bpr_batch_size': 1024,\n",
            " 'decay': 0.0001,\n",
            " 'dropout': 0,\n",
            " 'keep_prob': 0.6,\n",
            " 'latent_dim_rec': 64,\n",
            " 'lightGCN_n_layers': 10,\n",
            " 'lr': 0.001,\n",
            " 'multicore': 0,\n",
            " 'pretrain': 0,\n",
            " 'test_u_batch_size': 3}\n",
            "cores for test: 1\n",
            "comment: lgn\n",
            "tensorboard: 1\n",
            "LOAD: 0\n",
            "Weight path: ./checkpoints\n",
            "Test Topks: [20]\n",
            "using bpr loss\n",
            "===========end===================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procedure.py"
      ],
      "metadata": {
        "id": "Y_OLWnHZchXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Mar 1, 2020\n",
        "Pytorch Implementation of LightGCN in\n",
        "Xiangnan He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n",
        "@author: Jianbai Ye (gusye@mail.ustc.edu.cn)\n",
        "\n",
        "Design training and test process\n",
        "'''\n",
        "# import world\n",
        "import numpy as np\n",
        "import torch\n",
        "# import utils\n",
        "# import dataloader\n",
        "from pprint import pprint\n",
        "# from utils import timer\n",
        "import time\n",
        "# from time import time\n",
        "from tqdm import tqdm\n",
        "# import model\n",
        "import multiprocessing\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "CORES = multiprocessing.cpu_count() // 2\n",
        "\n",
        "\n",
        "def BPR_train_original(dataset, recommend_model, loss_class, epoch, neg_k=1, w=None):\n",
        "    Recmodel = recommend_model\n",
        "    Recmodel.train()\n",
        "    bpr: BPRLoss = loss_class\n",
        "    \n",
        "    with timer(name=\"Sample\"):\n",
        "        S = UniformSample_original(dataset)\n",
        "    users = torch.Tensor(S[:, 0]).long()\n",
        "    posItems = torch.Tensor(S[:, 1]).long()\n",
        "    negItems = torch.Tensor(S[:, 2]).long()\n",
        "\n",
        "    users = users.to(device)\n",
        "    posItems = posItems.to(device)\n",
        "    negItems = negItems.to(device)\n",
        "    users, posItems, negItems = shuffle(users, posItems, negItems)\n",
        "    total_batch = len(users) // config['bpr_batch_size'] + 1\n",
        "    aver_loss = 0.\n",
        "    for (batch_i,\n",
        "         (batch_users,\n",
        "          batch_pos,\n",
        "          batch_neg)) in enumerate(minibatch(users,\n",
        "                                                   posItems,\n",
        "                                                   negItems,\n",
        "                                                   batch_size=config['bpr_batch_size'])):\n",
        "        cri = bpr.stageOne(batch_users, batch_pos, batch_neg)\n",
        "        aver_loss += cri\n",
        "        if tensorboard:\n",
        "            w.add_scalar(f'BPRLoss/BPR', cri, epoch * int(len(users) / config['bpr_batch_size']) + batch_i)\n",
        "    aver_loss = aver_loss / total_batch\n",
        "    time_info = timer.dict()\n",
        "    timer.zero()\n",
        "    return f\"loss{aver_loss:.3f}-{time_info}\"\n",
        "    \n",
        "    \n",
        "def test_one_batch(X):\n",
        "    sorted_items = X[0].numpy()\n",
        "    groundTrue = X[1]\n",
        "    r = getLabel(groundTrue, sorted_items)\n",
        "    pre, recall, ndcg = [], [], []\n",
        "    for k in topks:\n",
        "        ret = RecallPrecision_ATk(groundTrue, r, k)\n",
        "        pre.append(ret['precision'])\n",
        "        recall.append(ret['recall'])\n",
        "        ndcg.append(NDCGatK_r(groundTrue,r,k))\n",
        "    return {'recall':np.array(recall), \n",
        "            'precision':np.array(pre), \n",
        "            'ndcg':np.array(ndcg)}\n",
        "        \n",
        "            \n",
        "def Test(dataset, Recmodel, epoch, w=None, multicore=0):\n",
        "    u_batch_size = config['test_u_batch_size']\n",
        "    dataset: BasicDataset\n",
        "    testDict: dict = dataset.testDict\n",
        "    Recmodel: LightGCN\n",
        "    # eval mode with no dropout\n",
        "    Recmodel = Recmodel.eval()\n",
        "    max_K = max(topks)\n",
        "    if multicore == 1:\n",
        "        pool = multiprocessing.Pool(CORES)\n",
        "    results = {'precision': np.zeros(len(topks)),\n",
        "               'recall': np.zeros(len(topks)),\n",
        "               'ndcg': np.zeros(len(topks))}\n",
        "    with torch.no_grad():\n",
        "        users = list(testDict.keys())\n",
        "        try:\n",
        "            assert u_batch_size <= len(users) / 10\n",
        "        except AssertionError:\n",
        "            print(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n",
        "        users_list = []\n",
        "        rating_list = []\n",
        "        groundTrue_list = []\n",
        "        # auc_record = []\n",
        "        # ratings = []\n",
        "        total_batch = len(users) // u_batch_size + 1\n",
        "        for batch_users in minibatch(users, batch_size=u_batch_size):\n",
        "            allPos = dataset.getUserPosItems(batch_users)\n",
        "            groundTrue = [testDict[u] for u in batch_users]\n",
        "            batch_users_gpu = torch.Tensor(batch_users).long()\n",
        "            batch_users_gpu = batch_users_gpu.to(device)\n",
        "\n",
        "            rating = Recmodel.getUsersRating(batch_users_gpu)\n",
        "            #rating = rating.cpu()\n",
        "            exclude_index = []\n",
        "            exclude_items = []\n",
        "            for range_i, items in enumerate(allPos):\n",
        "                exclude_index.extend([range_i] * len(items))\n",
        "                exclude_items.extend(items)\n",
        "            rating[exclude_index, exclude_items] = -(1<<10)\n",
        "            _, rating_K = torch.topk(rating, k=max_K)\n",
        "            rating = rating.cpu().numpy()\n",
        "            # aucs = [ \n",
        "            #         utils.AUC(rating[i],\n",
        "            #                   dataset, \n",
        "            #                   test_data) for i, test_data in enumerate(groundTrue)\n",
        "            #     ]\n",
        "            # auc_record.extend(aucs)\n",
        "            del rating\n",
        "            users_list.append(batch_users)\n",
        "            rating_list.append(rating_K.cpu())\n",
        "            groundTrue_list.append(groundTrue)\n",
        "        assert total_batch >= len(users_list)\n",
        "        X = zip(rating_list, groundTrue_list)\n",
        "        if multicore == 1:\n",
        "            pre_results = pool.map(test_one_batch, X)\n",
        "        else:\n",
        "            pre_results = []\n",
        "            for x in X:\n",
        "                pre_results.append(test_one_batch(x))\n",
        "        scale = float(u_batch_size/len(users))\n",
        "        for result in pre_results:\n",
        "            results['recall'] += result['recall']\n",
        "            results['precision'] += result['precision']\n",
        "            results['ndcg'] += result['ndcg']\n",
        "        results['recall'] /= float(len(users))\n",
        "        results['precision'] /= float(len(users))\n",
        "        results['ndcg'] /= float(len(users))\n",
        "        # results['auc'] = np.mean(auc_record)\n",
        "        if tensorboard:\n",
        "            w.add_scalars(f'Test/Recall@{topks}',\n",
        "                          {str(topks[i]): results['recall'][i] for i in range(len(topks))}, epoch)\n",
        "            w.add_scalars(f'Test/Precision@{topks}',\n",
        "                          {str(topks[i]): results['precision'][i] for i in range(len(topks))}, epoch)\n",
        "            w.add_scalars(f'Test/NDCG@{topks}',\n",
        "                          {str(topks[i]): results['ndcg'][i] for i in range(len(topks))}, epoch)\n",
        "        if multicore == 1:\n",
        "            pool.close()\n",
        "        print(results)\n",
        "        return results"
      ],
      "metadata": {
        "id": "YXC2atqDchBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main.py"
      ],
      "metadata": {
        "id": "e8DbjpktG_Yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gowalla"
      ],
      "metadata": {
        "id": "r-jnGdflykLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --target=$nb_path tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "hVrlamO1dqy2",
        "outputId": "f438f3de-913e-43f5-e349-02dcdfb3530f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Using cached tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "Collecting protobuf>=3.8.0\n",
            "  Using cached protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting six\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, protobuf, numpy, tensorboardX\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.5 protobuf-3.19.4 six-1.16.0 tensorboardX-2.5\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/google already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/tensorboardX-2.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/protobuf-3.19.4-py3.7-nspkg.pth already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/protobuf-3.19.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/numpy-1.21.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/six-1.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/tensorboardX already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import world\n",
        "# import utils\n",
        "# from world import cprint\n",
        "import torch\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "import time\n",
        "# import Procedure\n",
        "from os.path import join\n",
        "# ==============================\n",
        "set_seed(seed)\n",
        "print(\">>SEED:\", seed)\n",
        "# ==============================\n",
        "# import register\n",
        "# from register import dataset\n",
        "\n",
        "Recmodel = MODELS[model_name](config, dataset)\n",
        "Recmodel = Recmodel.to(device)\n",
        "bpr = BPRLoss(Recmodel, config)\n",
        "\n",
        "weight_file = getFileName()\n",
        "print(f\"load and save to {weight_file}\")\n",
        "if LOAD:\n",
        "    try:\n",
        "        Recmodel.load_state_dict(torch.load(weight_file,map_location=torch.device('cpu')))\n",
        "        cprint(f\"loaded model weights from {weight_file}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"{weight_file} not exists, start from beginning\")\n",
        "Neg_k = 1\n",
        "\n",
        "# init tensorboard\n",
        "if tensorboard:\n",
        "    w : SummaryWriter = SummaryWriter(\n",
        "                                    join(BOARD_PATH, time.strftime(\"%m-%d-%Hh%Mm%Ss-\") + \"-\" + comment)\n",
        "                                    )\n",
        "else:\n",
        "    w = None\n",
        "    cprint(\"not enable tensorflowboard\")\n",
        "\n",
        "try:\n",
        "    for epoch in range(TRAIN_epochs):\n",
        "        start = time.time()\n",
        "        if epoch %10 == 0:\n",
        "            cprint(\"[TEST]\")\n",
        "            Test(dataset, Recmodel, epoch, w, config['multicore'])\n",
        "        output_information = BPR_train_original(dataset, Recmodel, bpr, epoch, neg_k=Neg_k,w=w)\n",
        "        print(f'EPOCH[{epoch+1}/{TRAIN_epochs}] {output_information}')\n",
        "        torch.save(Recmodel.state_dict(), weight_file)\n",
        "finally:\n",
        "    if tensorboard:\n",
        "        w.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjc1V6afdRmt",
        "outputId": "7b5296b2-c136-434e-bf22-3016260b74c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>SEED: 2020\n",
            "\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n",
            "loading adjacency matrix\n",
            "generating adjacency matrix\n",
            "costing 0.07744860649108887s, saved norm_mat...\n",
            "don't split the matrix\n",
            "lgn is already to go(dropout:0)\n",
            "load and save to /content/mnt/Shareddrives/247 Project/code/checkpoints/lgn-<__main__.Loader object at 0x7fe24cd02090>-10-64.pth.tar\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:355: RuntimeWarning: divide by zero encountered in power\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'precision': array([0.04166667]), 'recall': array([0.07837921]), 'ndcg': array([0.11853127])}\n",
            "EPOCH[1/200] loss0.691-|Sample:0.38|\n",
            "EPOCH[2/200] loss0.690-|Sample:0.37|\n",
            "EPOCH[3/200] loss0.687-|Sample:0.19|\n",
            "EPOCH[4/200] loss0.682-|Sample:0.31|\n",
            "EPOCH[5/200] loss0.676-|Sample:0.36|\n",
            "EPOCH[6/200] loss0.667-|Sample:0.39|\n",
            "EPOCH[7/200] loss0.655-|Sample:0.42|\n",
            "EPOCH[8/200] loss0.643-|Sample:0.39|\n",
            "EPOCH[9/200] loss0.633-|Sample:0.40|\n",
            "EPOCH[10/200] loss0.619-|Sample:0.53|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.06666667]), 'recall': array([0.17668812]), 'ndcg': array([0.15046021])}\n",
            "EPOCH[11/200] loss0.609-|Sample:0.35|\n",
            "EPOCH[12/200] loss0.596-|Sample:0.57|\n",
            "EPOCH[13/200] loss0.585-|Sample:0.50|\n",
            "EPOCH[14/200] loss0.573-|Sample:0.43|\n",
            "EPOCH[15/200] loss0.559-|Sample:0.40|\n",
            "EPOCH[16/200] loss0.557-|Sample:0.34|\n",
            "EPOCH[17/200] loss0.544-|Sample:0.42|\n",
            "EPOCH[18/200] loss0.540-|Sample:0.56|\n",
            "EPOCH[19/200] loss0.526-|Sample:0.17|\n",
            "EPOCH[20/200] loss0.515-|Sample:0.19|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.19663664]), 'ndcg': array([0.14514586])}\n",
            "EPOCH[21/200] loss0.517-|Sample:0.18|\n",
            "EPOCH[22/200] loss0.508-|Sample:0.18|\n",
            "EPOCH[23/200] loss0.499-|Sample:0.17|\n",
            "EPOCH[24/200] loss0.499-|Sample:0.18|\n",
            "EPOCH[25/200] loss0.484-|Sample:0.18|\n",
            "EPOCH[26/200] loss0.479-|Sample:0.27|\n",
            "EPOCH[27/200] loss0.476-|Sample:0.17|\n",
            "EPOCH[28/200] loss0.465-|Sample:0.19|\n",
            "EPOCH[29/200] loss0.459-|Sample:0.17|\n",
            "EPOCH[30/200] loss0.465-|Sample:0.17|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.199141]), 'ndcg': array([0.14845638])}\n",
            "EPOCH[31/200] loss0.455-|Sample:0.18|\n",
            "EPOCH[32/200] loss0.451-|Sample:0.19|\n",
            "EPOCH[33/200] loss0.447-|Sample:0.17|\n",
            "EPOCH[34/200] loss0.441-|Sample:0.28|\n",
            "EPOCH[35/200] loss0.435-|Sample:0.18|\n",
            "EPOCH[36/200] loss0.430-|Sample:0.17|\n",
            "EPOCH[37/200] loss0.430-|Sample:0.17|\n",
            "EPOCH[38/200] loss0.425-|Sample:0.17|\n",
            "EPOCH[39/200] loss0.418-|Sample:0.18|\n",
            "EPOCH[40/200] loss0.414-|Sample:0.17|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07166667]), 'recall': array([0.21706035]), 'ndcg': array([0.1571238])}\n",
            "EPOCH[41/200] loss0.414-|Sample:0.26|\n",
            "EPOCH[42/200] loss0.412-|Sample:0.18|\n",
            "EPOCH[43/200] loss0.402-|Sample:0.18|\n",
            "EPOCH[44/200] loss0.397-|Sample:0.17|\n",
            "EPOCH[45/200] loss0.405-|Sample:0.18|\n",
            "EPOCH[46/200] loss0.387-|Sample:0.18|\n",
            "EPOCH[47/200] loss0.379-|Sample:0.18|\n",
            "EPOCH[48/200] loss0.378-|Sample:0.18|\n",
            "EPOCH[49/200] loss0.371-|Sample:0.28|\n",
            "EPOCH[50/200] loss0.369-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07833333]), 'recall': array([0.22625382]), 'ndcg': array([0.1716714])}\n",
            "EPOCH[51/200] loss0.373-|Sample:0.17|\n",
            "EPOCH[52/200] loss0.364-|Sample:0.18|\n",
            "EPOCH[53/200] loss0.363-|Sample:0.18|\n",
            "EPOCH[54/200] loss0.361-|Sample:0.17|\n",
            "EPOCH[55/200] loss0.360-|Sample:0.18|\n",
            "EPOCH[56/200] loss0.351-|Sample:0.18|\n",
            "EPOCH[57/200] loss0.354-|Sample:0.28|\n",
            "EPOCH[58/200] loss0.352-|Sample:0.17|\n",
            "EPOCH[59/200] loss0.336-|Sample:0.17|\n",
            "EPOCH[60/200] loss0.341-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07833333]), 'recall': array([0.21117446]), 'ndcg': array([0.18030777])}\n",
            "EPOCH[61/200] loss0.350-|Sample:0.18|\n",
            "EPOCH[62/200] loss0.334-|Sample:0.18|\n",
            "EPOCH[63/200] loss0.339-|Sample:0.18|\n",
            "EPOCH[64/200] loss0.322-|Sample:0.17|\n",
            "EPOCH[65/200] loss0.326-|Sample:0.28|\n",
            "EPOCH[66/200] loss0.330-|Sample:0.17|\n",
            "EPOCH[67/200] loss0.321-|Sample:0.18|\n",
            "EPOCH[68/200] loss0.313-|Sample:0.17|\n",
            "EPOCH[69/200] loss0.319-|Sample:0.18|\n",
            "EPOCH[70/200] loss0.309-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07166667]), 'recall': array([0.15668743]), 'ndcg': array([0.16461479])}\n",
            "EPOCH[71/200] loss0.306-|Sample:0.18|\n",
            "EPOCH[72/200] loss0.313-|Sample:0.18|\n",
            "EPOCH[73/200] loss0.308-|Sample:0.27|\n",
            "EPOCH[74/200] loss0.300-|Sample:0.18|\n",
            "EPOCH[75/200] loss0.297-|Sample:0.17|\n",
            "EPOCH[76/200] loss0.302-|Sample:0.17|\n",
            "EPOCH[77/200] loss0.290-|Sample:0.18|\n",
            "EPOCH[78/200] loss0.295-|Sample:0.17|\n",
            "EPOCH[79/200] loss0.283-|Sample:0.17|\n",
            "EPOCH[80/200] loss0.288-|Sample:0.17|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.15591224]), 'ndcg': array([0.16402733])}\n",
            "EPOCH[81/200] loss0.287-|Sample:0.27|\n",
            "EPOCH[82/200] loss0.286-|Sample:0.18|\n",
            "EPOCH[83/200] loss0.282-|Sample:0.17|\n",
            "EPOCH[84/200] loss0.278-|Sample:0.17|\n",
            "EPOCH[85/200] loss0.280-|Sample:0.19|\n",
            "EPOCH[86/200] loss0.273-|Sample:0.17|\n",
            "EPOCH[87/200] loss0.273-|Sample:0.17|\n",
            "EPOCH[88/200] loss0.268-|Sample:0.28|\n",
            "EPOCH[89/200] loss0.268-|Sample:0.18|\n",
            "EPOCH[90/200] loss0.262-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.06833333]), 'recall': array([0.15550072]), 'ndcg': array([0.16257698])}\n",
            "EPOCH[91/200] loss0.264-|Sample:0.18|\n",
            "EPOCH[92/200] loss0.253-|Sample:0.18|\n",
            "EPOCH[93/200] loss0.268-|Sample:0.17|\n",
            "EPOCH[94/200] loss0.271-|Sample:0.17|\n",
            "EPOCH[95/200] loss0.259-|Sample:0.18|\n",
            "EPOCH[96/200] loss0.256-|Sample:0.27|\n",
            "EPOCH[97/200] loss0.253-|Sample:0.17|\n",
            "EPOCH[98/200] loss0.255-|Sample:0.17|\n",
            "EPOCH[99/200] loss0.251-|Sample:0.17|\n",
            "EPOCH[100/200] loss0.246-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.18883405]), 'ndcg': array([0.16965887])}\n",
            "EPOCH[101/200] loss0.250-|Sample:0.17|\n",
            "EPOCH[102/200] loss0.242-|Sample:0.19|\n",
            "EPOCH[103/200] loss0.254-|Sample:0.17|\n",
            "EPOCH[104/200] loss0.250-|Sample:0.27|\n",
            "EPOCH[105/200] loss0.247-|Sample:0.19|\n",
            "EPOCH[106/200] loss0.256-|Sample:0.18|\n",
            "EPOCH[107/200] loss0.236-|Sample:0.17|\n",
            "EPOCH[108/200] loss0.239-|Sample:0.17|\n",
            "EPOCH[109/200] loss0.233-|Sample:0.17|\n",
            "EPOCH[110/200] loss0.248-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.18883405]), 'ndcg': array([0.17058862])}\n",
            "EPOCH[111/200] loss0.240-|Sample:0.18|\n",
            "EPOCH[112/200] loss0.231-|Sample:0.28|\n",
            "EPOCH[113/200] loss0.243-|Sample:0.18|\n",
            "EPOCH[114/200] loss0.227-|Sample:0.17|\n",
            "EPOCH[115/200] loss0.224-|Sample:0.18|\n",
            "EPOCH[116/200] loss0.231-|Sample:0.17|\n",
            "EPOCH[117/200] loss0.233-|Sample:0.17|\n",
            "EPOCH[118/200] loss0.222-|Sample:0.17|\n",
            "EPOCH[119/200] loss0.219-|Sample:0.17|\n",
            "EPOCH[120/200] loss0.221-|Sample:0.27|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07166667]), 'recall': array([0.18973495]), 'ndcg': array([0.17288635])}\n",
            "EPOCH[121/200] loss0.220-|Sample:0.18|\n",
            "EPOCH[122/200] loss0.215-|Sample:0.18|\n",
            "EPOCH[123/200] loss0.217-|Sample:0.17|\n",
            "EPOCH[124/200] loss0.215-|Sample:0.18|\n",
            "EPOCH[125/200] loss0.212-|Sample:0.18|\n",
            "EPOCH[126/200] loss0.216-|Sample:0.17|\n",
            "EPOCH[127/200] loss0.208-|Sample:0.18|\n",
            "EPOCH[128/200] loss0.212-|Sample:0.27|\n",
            "EPOCH[129/200] loss0.213-|Sample:0.17|\n",
            "EPOCH[130/200] loss0.209-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07166667]), 'recall': array([0.18973495]), 'ndcg': array([0.17425839])}\n",
            "EPOCH[131/200] loss0.209-|Sample:0.17|\n",
            "EPOCH[132/200] loss0.209-|Sample:0.18|\n",
            "EPOCH[133/200] loss0.206-|Sample:0.17|\n",
            "EPOCH[134/200] loss0.207-|Sample:0.18|\n",
            "EPOCH[135/200] loss0.213-|Sample:0.29|\n",
            "EPOCH[136/200] loss0.203-|Sample:0.17|\n",
            "EPOCH[137/200] loss0.201-|Sample:0.18|\n",
            "EPOCH[138/200] loss0.208-|Sample:0.17|\n",
            "EPOCH[139/200] loss0.192-|Sample:0.18|\n",
            "EPOCH[140/200] loss0.202-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07333333]), 'recall': array([0.19028406]), 'ndcg': array([0.17752711])}\n",
            "EPOCH[141/200] loss0.199-|Sample:0.18|\n",
            "EPOCH[142/200] loss0.200-|Sample:0.18|\n",
            "EPOCH[143/200] loss0.199-|Sample:0.28|\n",
            "EPOCH[144/200] loss0.206-|Sample:0.17|\n",
            "EPOCH[145/200] loss0.206-|Sample:0.17|\n",
            "EPOCH[146/200] loss0.191-|Sample:0.17|\n",
            "EPOCH[147/200] loss0.192-|Sample:0.17|\n",
            "EPOCH[148/200] loss0.191-|Sample:0.19|\n",
            "EPOCH[149/200] loss0.191-|Sample:0.17|\n",
            "EPOCH[150/200] loss0.197-|Sample:0.17|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.18522779]), 'ndcg': array([0.17416925])}\n",
            "EPOCH[151/200] loss0.186-|Sample:0.28|\n",
            "EPOCH[152/200] loss0.190-|Sample:0.18|\n",
            "EPOCH[153/200] loss0.191-|Sample:0.17|\n",
            "EPOCH[154/200] loss0.178-|Sample:0.17|\n",
            "EPOCH[155/200] loss0.188-|Sample:0.19|\n",
            "EPOCH[156/200] loss0.183-|Sample:0.17|\n",
            "EPOCH[157/200] loss0.183-|Sample:0.17|\n",
            "EPOCH[158/200] loss0.186-|Sample:0.18|\n",
            "EPOCH[159/200] loss0.180-|Sample:0.28|\n",
            "EPOCH[160/200] loss0.183-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07166667]), 'recall': array([0.18856113]), 'ndcg': array([0.17611264])}\n",
            "EPOCH[161/200] loss0.185-|Sample:0.18|\n",
            "EPOCH[162/200] loss0.187-|Sample:0.21|\n",
            "EPOCH[163/200] loss0.183-|Sample:0.17|\n",
            "EPOCH[164/200] loss0.184-|Sample:0.17|\n",
            "EPOCH[165/200] loss0.175-|Sample:0.56|\n",
            "EPOCH[166/200] loss0.178-|Sample:0.52|\n",
            "EPOCH[167/200] loss0.170-|Sample:0.67|\n",
            "EPOCH[168/200] loss0.172-|Sample:0.17|\n",
            "EPOCH[169/200] loss0.172-|Sample:0.19|\n",
            "EPOCH[170/200] loss0.175-|Sample:0.17|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.18850139]), 'ndcg': array([0.17515413])}\n",
            "EPOCH[171/200] loss0.169-|Sample:0.18|\n",
            "EPOCH[172/200] loss0.175-|Sample:0.17|\n",
            "EPOCH[173/200] loss0.170-|Sample:0.17|\n",
            "EPOCH[174/200] loss0.169-|Sample:0.28|\n",
            "EPOCH[175/200] loss0.169-|Sample:0.17|\n",
            "EPOCH[176/200] loss0.165-|Sample:0.17|\n",
            "EPOCH[177/200] loss0.172-|Sample:0.17|\n",
            "EPOCH[178/200] loss0.170-|Sample:0.17|\n",
            "EPOCH[179/200] loss0.164-|Sample:0.17|\n",
            "EPOCH[180/200] loss0.170-|Sample:0.18|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.06666667]), 'recall': array([0.18754075]), 'ndcg': array([0.17500385])}\n",
            "EPOCH[181/200] loss0.169-|Sample:0.18|\n",
            "EPOCH[182/200] loss0.162-|Sample:0.28|\n",
            "EPOCH[183/200] loss0.161-|Sample:0.17|\n",
            "EPOCH[184/200] loss0.165-|Sample:0.17|\n",
            "EPOCH[185/200] loss0.165-|Sample:0.17|\n",
            "EPOCH[186/200] loss0.163-|Sample:0.17|\n",
            "EPOCH[187/200] loss0.166-|Sample:0.17|\n",
            "EPOCH[188/200] loss0.165-|Sample:0.18|\n",
            "EPOCH[189/200] loss0.156-|Sample:0.17|\n",
            "EPOCH[190/200] loss0.158-|Sample:0.28|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "{'precision': array([0.07]), 'recall': array([0.22246139]), 'ndcg': array([0.18544439])}\n",
            "EPOCH[191/200] loss0.157-|Sample:0.18|\n",
            "EPOCH[192/200] loss0.156-|Sample:0.18|\n",
            "EPOCH[193/200] loss0.165-|Sample:0.18|\n",
            "EPOCH[194/200] loss0.165-|Sample:0.17|\n",
            "EPOCH[195/200] loss0.161-|Sample:0.18|\n",
            "EPOCH[196/200] loss0.158-|Sample:0.17|\n",
            "EPOCH[197/200] loss0.158-|Sample:0.18|\n",
            "EPOCH[198/200] loss0.156-|Sample:0.28|\n",
            "EPOCH[199/200] loss0.155-|Sample:0.17|\n",
            "EPOCH[200/200] loss0.154-|Sample:0.17|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "375VRr1-dkeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}